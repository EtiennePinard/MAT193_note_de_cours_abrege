\unnumsec{Espace vectoriel de dimension finie}
\begin{definition}
      Un ensemble $V$ est un espace vectoriel sur un corps $F$
      si \begin{enumerate}[1)]
            \item $V$ est fermé sous l'addition, c-à-d 
            \[ \forall \ (v_1, v_2 \in V), \ v_1 + v_2 \in V \]
            \item $V$ est fermé sous la multiplication par un scalaire, c-à-d, \[ \forall \ (v \in V, \ \alpha \in F), \ \alpha v \in V \]
      \end{enumerate}
      Les éléments de $V$ sont appelés \guillemetleft \ vecteurs \guillemetright.
\end{definition}
\begin{remark}
      L'espace vectoriel le plus simple est $V = \{ 0_v \}$, où $0_v$ est l'élément nul.
\end{remark}
\begin{definition}
      Un scalaire $\alpha$ est un élément du corps $F$ associé à l'espace vectoriel.
\end{definition}
\begin{note}
    Dans notre cas, ce corps est soit les nombres réels $\R$ ou les nombres complexes $\C$
\end{note}

\subsection{Base d'un espace vectoriel}
Une base d'un espace vectoriel est une manière d'encoder l'information d'un vecteur. Commençons par définir les concepts utilisés dans la définition d'une base. Cette sous-section va utiliser ces deux objets, soit $V$ un espace vectoriel et $S = \{ u_1, u_2, \ldots, u_m \} \subset V$

\subsubsection{Combinaison linéaire}
\begin{definition} Une combinaison linéaire de $S$ est la somme des éléments $S$ multipliés par des scalaires $\alpha_1, \alpha_2, \ldots, \alpha_m$, c.-à-d. 
\[ \sum_{j = 1}^{n}\alpha_j u_j = \alpha_1 u_1 + \alpha_2 u_2 + \ldots + \alpha_m u_m \]
\end{definition}

\subsubsection{Ensemble générateur}
\begin{definition}
      $S$ est un ensemble générateur de $V$ si on peut écrire tout vecteur de $V$ comme une combinaison linéaire de $S$, c.-à.-d.
      \[ \forall v \in V  \ \exists \alpha_1, \ldots, \alpha_m \text{ scalaire t.q. } v = \alpha_1 u_1 + \ldots + \alpha_m u_m\]
\end{definition}

\subsubsection{Indépendance linéaire}
\begin{definition}
      $S$ est linéairement indépendant si la combinaison linéaire de $S$ donne 0 seulement quand tous les scalaires sont 0, c.-à.-d.
      \[
            \alpha_1 u_1 + \alpha_2 u_2 + \ldots + \alpha_n u_n = 0_v \implies \alpha_1 = \alpha_2 = \ldots = \alpha_n = 0
      \]
      sinon $S$ est linéairement dépendant ou lié.
\end{definition}

\subsubsection{Déterminer l'indépendance linéaire d'un ensemble de vecteurs colonne}
Supposons que $u_j \in S$ est un vecteur colonne de taille $n \times 1$. Notons k-ème élément du vecteur $u_j$ comme $u_{jk}$. Trouvons une condition pour l'indépendance linéaire de $S$
\begin{align*}
    & \alpha_1 u_1 + \alpha_2 u_2 + \ldots + \alpha_n u_n = \mathbb{O} \\[0.5em]
    \iff & \alpha_1 \begin{pmatrix}
                  u_{11} \\
                  u_{12} \\
                  \vdots   \\
                  u_{1n}
            \end{pmatrix} + \alpha_2 \begin{pmatrix}
                  u_{21} \\
                  u_{22} \\
                  \vdots   \\
                  u_{2n}
            \end{pmatrix} + \dots + \alpha_n \begin{pmatrix}
                  u_{n1} \\
                  u_{n2} \\
                  \vdots   \\
                  u_{nn}
            \end{pmatrix} = \mathbb{O} \\[0.5em]
    \iff & \begin{pmatrix}
        \alpha_1 u_{11} + \alpha_2 u_{12} + \ldots + \alpha_n u_{1n} \\
        \alpha_1 u_{21} + \alpha_2 u_{22} + \ldots + \alpha_n u_{2n} \\
        \vdots \\
        \alpha_1 u_{n1} + \alpha_2 u_{n2} + \ldots + \alpha_n u_{nn} \\
    \end{pmatrix} = \mathbb{O} \\[0.5em]
     \iff & \begin{pmatrix}
        u_{11} & u_{12} & \ldots & u_{1n} \\
        u_{21} & u_{22} & \ldots & u_{2n} \\
        \vdots & & \ddots & \vdots\\
        u_{n1} & u_{n2} & \ldots & u_{nn} \\
    \end{pmatrix} \begin{pmatrix}
        \alpha_1 \\
        \alpha_2 \\
        \vdots \\
        \alpha_n
    \end{pmatrix} = \mathbb{O}
\end{align*}
Posons $M$ comme la matrice de coefficient du système. Nous pouvons exprimer la matrice augmentée du système comme tel: $(M | \mathbb{O})$. Nous pouvons utiliser le théorème \ref{inversible_rank_kernel_thm} pour arriver à deux conclusions: 
\begin{enumerate}[1.]
      \item $\text{rg}(M) < m \implies S$ est lié.
      \item $\text{rg}(M^T) = m \implies S$ est linéairement indépendant.
\end{enumerate}
% Add the other way of doing M^T to make it easier
Note: Échelonner $M^T$ donne des résultats plus simples à interpréter puisque
$0 \leq \text{rg}(M^T) \leq m$. Si la forme échelonnée de $M^T$ contient une ligne nulle,
on sait automatiquement que $S$ est lié, ce qui n'est pas le cas si la forme échelonnée de $M$ 
contient une ligne nulle.

\subsubsection{Base d'un espace vectoriel}
\begin{definition}
      Un ensemble $B = \{ u_1, u_2, \ldots, u_n \} \subset V$, est une base de $V$ si
      $B$ est un ensemble générateur de $V$ et $B$ est linéairement indépendant.
\end{definition}
\begin{definition}
      La dimension de $V$ est $\dim_F V = |B|$, ou $|B|$ est le nombre d'éléments dans la base $B$
      de $V$ et $F$ est le corps de l'espace vectoriel.
\end{definition}
\begin{theorem}
      Le nombre de vecteurs dans une base de $V$ ne dépend pas de la base choisie.
\end{theorem}
\begin{remark}
      Le théorème qui précède nous permet de définir la dimension de $V$ comme étant le
      nombre de vecteurs dans une base de $V$, puisque toutes les bases de $V$ contiennent
      le même nombre de vecteurs.
\end{remark}
\paragraph{Base canonique:}
Un espace vectoriel a souvent une base canonique, soit une base plus naturelle à utiliser.
Par exemple, la base canonique de $\R^n$ est $S = \{ e_1, e_2, \ldots, e_n \}$.
On peut donc dire que $\dim_\R \R^n = n$

\subsubsection{Représentation d'un vecteur dans une base}
Soit $V$ un espace vectoriel avec $\dim_F V = n$, et $B = \{ u_1, u_2, \ldots, u_n \}$ une base de $V$. \\
Cela veut dire que $ \forall \ v \in V  \ \exists \ \alpha_1, \ldots, \alpha_n \text{ scalaire t.q. } v = \alpha_1 u_1 + \ldots + \alpha_n u_n$
\begin{definition}
      Les scalaires $\alpha_1, \ldots, \alpha_n$ sont appellés les coordonnées de $v$ dans la base $B$.
\end{definition}
\begin{definition}
      La représentation de $v$ dans la base $B$ est notée $[v]_B$ et est exprimée
      \[
            [v]_B = \begin{pmatrix}
                  \alpha_1 \\
                  \alpha_2 \\
                  \vdots   \\
                  \alpha_n
            \end{pmatrix} \in F^n
      \]
\end{definition}

\subsubsection{Propriétés d'une base d'un espace vectoriel}
\begin{enumerate}[a)]
      \item Soit $v_1, v_2 \in V$, alors $v_1 = v_2 \iff [v_1]_B = [v_2]_B$
      \item La représentation dans une base est linéaire, ce qui veut dire qu'elle satisfait ces deux propriétés:
            \begin{enumerate}[1.]
                  \item $[v_1 + v_2]_B = [v_1]_B + [v_2]_B$
                  \item $[\alpha v]_B = \alpha [v]_B$
            \end{enumerate}
      \item Soit $S = \{ u_1, u_2, \ldots, u_n \} \subset V$ \\
            $S^\prime = \{ [u_1]_B, [u_2]_B, \ldots, [u_n]_B \}$ linéairement indépendant $\iff S$ linéairement indépendant
\end{enumerate}

\subsection{Matrice de changement de base}

Soit $V$ un espace vectoriel avec $\dim_F V = n$ \\
Soit $B = \left\{ u_1, u_2, \dots, u_n \right\}$ et $B^\prime = \left\{ u_1^\prime, u_2^\prime, \dots, u_n^\prime \right\}$ deux bases de $V$ 
\begin{definition}
      La matrice de changement (ou de passage) de la base $B^\prime$ à $B$ est notée
      $P^{B^\prime}_B$ et est donnée par $P^{B^\prime}_B = \begin{bmatrix}
           [ u_1^\prime]_B & [u_2^\prime]_B & \dots & [u_n^\prime]_B
      \end{bmatrix}$. Les bases sont liées par la matrice de passage par l'équation 
      \[P^{B^\prime}_B [v]_{B^\prime} = [v]_B\]
\end{definition}
Note: Il a plusieurs notations pour la matrice de passage. La notation utilisée
dans cette définition est équivalente à $\underset{B^\prime \to B}{P}$
\begin{lemma}
      La matrice de passage de $B$ à $B^\prime$ est l'inverse de la matrice de passage de 
      $B^\prime$ à $B$, soit \[P^B_{B^\prime} = \left(P^{B^\prime}_B\right)^{-1}\]
\end{lemma}
\begin{lemma}
      La matrice de passage est unique.
\end{lemma}

\subsection{L'indépendance linéaire des fonctions} % Retraviller le formatting
Soit $V$ l'espace vectoriel de toutes les fonctions $f: \R \to \R$ avec des scalaire réels \\
La dimensions de $V$ est infinie et il n'y a pas de base dans $V$ \\
Soit ${v_1, v_2, \dots, v_n} \subset V$ et $S = \text{span}\{v_1, v_2, \dots, v_n\}$. \\
Pour déterminer l'indépendance linéaire de $S$, considérons $F(x) = \alpha_1 v_1 + \alpha_2 v_2 + \dots + \alpha_n v_n$ \\
Supposons que $S$ est linéairement indépendant, c-à-d que $F(x) = 0 \ \forall \ x \in \R$ \\
Il faut ensuite prendre $x_1, x_2, \dots, x_n$ valeurs et évaluer $F(x_1), F(x_2), \dots, F(x_n)$. \\
Le résultat va être $n$ équations de la forme $\alpha_1 v_1(x_j)+ \alpha_2 v_2(x_j)+ \dots + \alpha_n v_n(x_j) = 0$ \\
\underline{Important:} Pour que la méthode fonctionne, il faut que les $n$ équations soient uniques. 
En d'autres termes, si $F(a) = F(b)$ alors ni $a$ ni $b$ ne peuvent faire partie des $n$ valeurs choisies. \\
Maintenant, il faut résoudre le système homogène $\begin{cases}
      \alpha_1 v_1(x_1)+ \alpha_2 v_2(x_1)+ \dots + \alpha_n v_n(x_1) = 0 \\
      \alpha_1 v_1(x_2)+ \alpha_2 v_2(x_2)+ \dots + \alpha_n v_n(x_2) = 0 \\
      \vdots \\
      \alpha_1 v_1(x_n)+ \alpha_2 v_2(x_n)+ \dots + \alpha_n v_n(x_n) = 0 \\
\end{cases}$ \\
La résolution du système va aboutir à deux possibilités: \begin{enumerate}
      \item Le système homogène a seulement la solution nulle $\implies S$ est linéairement indépendant
      \item Le système homogène a une infinité de solutions. Dans ce cas, on doit construire
      une nouvelle fonction $\hat{F}(x) = \beta_1 v_1 + \beta_2 v_2 + \dots + \beta_n v_n$ où $\beta_1, \beta_2, \dots, \beta_n$
      est une solution non-nulle du système. Il faut alors démontrer que $\hat{F}(x) = 0 \ \forall \ x \in \R$. 
      Si c'est le cas, alors on peut conclure que $S$ est lié.
\end{enumerate}

\subsection{Espaces vectoriels munis d'un produit scalaire}

\subsubsection{Produit scalaire réel}
Soit $V$ un espace vectoriel réel
\begin{definition}
      \label{scpr_Real}
      Un produit scalaire dans $V$ est une application $\myfunc{\scpr{\cdot}{\cdot}}{V \times V}{\R}{u, v \in V}{\scpr{u}{v}}$, t.q.
      \begin{align*}
            &1. \quad \scpr{u}{v} = \scpr{v}{u}& &2. \quad \scpr{u}{v + \alpha w} = \scpr{u}{v} + \alpha \scpr{u}{w}& &3. \quad \scpr{u}{u} \geq 0 \ \text{et} \ \scpr{u}{u} = 0 \iff u = 0&
      \end{align*}
\end{definition}
\begin{remark}
      Le produit scalaire réel n'est pas unique. 
\end{remark}
\begin{definition}
      Une matrice $A \in M_{n \times n}(\R)$ est définie positive si $x^T A x > 0 \ \forall \ x \neq 0, \ x \in R^n$
\end{definition}
\begin{lemma}
      Soit $x,y \in \R^n$ et $A \in M_{n \times n}(\R)$ symétrique et définie positive, alors tout produit scalaire dans $\R^n$ 
      peut être écrit comme tel, $\scpr{x}{y} = x^T A y$. On note ce produit scalaire modifié $\scpr{u}{v}_A$
\end{lemma}
\begin{definition}
      Le cas où $A = I \implies \scpr{x}{y} = x^T I y = x^T y$ est appellé le produit scalaire 
      canonique (ou euclédien) dans $\R^n$
\end{definition}

\subsubsection{Produit scalaire complexe}
Soit $V$ un espace vectoriel complexe. Le produit scalaire complexe est défini de manière
similaire au produit scalaire réel [\ref{scpr_Real}] à l'exception d'une restriction de plus.
\begin{definition}
      Un produit scalaire dans $V$ est une application $\myfunc{\scpr{\cdot}{\cdot}}{V \times V}{\C}{u, v \in V}{\scpr{u}{v}}$, t.q.
      \begin{align*}
            &1. \quad \scpr{u}{v} = \scpr{v}{u}^*& &2. \quad \scpr{u}{v + \alpha w} = \scpr{u}{v} + \alpha \scpr{u}{w}& &3. \quad \scpr{u}{u} \geq 0 \ \text{et} \ \scpr{u}{u} = 0 \iff u = 0&
      \end{align*}
\end{definition}
\begin{remark}
      La définition implique que $\scpr{\alpha v + \beta u}{w} = \alpha^* \scpr{v}{w} + \beta^* \scpr{u}{w}$
\end{remark}
\begin{lemma}
      Soit $x,y \in \C^n$ et $A \in M_{n \times n}(\C)$ hermitienne et définie positive, alors tout produit scalaire dans $\C^n$ 
      peut être écrit comme tel, $\scpr{x}{y}_A = x^\dagger A y$.
\end{lemma}
\begin{definition}
      Le produit scalaire canonique dans $\C^n$ est $\scpr{x}{y} = x^\dagger y$
\end{definition}

\subsubsection{Orthogonalité entre deux vecteurs}
Soit $V$ un espace vectoriel muni d'un produit scalaire
\begin{definition}
      $x, y \in V$ sont orthogonaux $\iff \scpr{x}{y} = 0$. Alors on note $x \perp y$.
\end{definition}
\begin{remark}
      L'orthogonalité entre deux vecteurs est toujours par rapport au produit scalaire utilisé
\end{remark}

\subsubsection{Norme d'un vecteur}
Soit $V$ un espace vectoriel muni d'un produit scalaire
\begin{definition}
      La norme de $x \in V$ par rapport au produit scalaire dans $V$ est $\norm{x} = \sqrt{\scpr{x}{x}}$
\end{definition}
\begin{remark}
      La norme d'un vecteur change par rapport au produit scalaire utilisé
\end{remark}
\begin{definition}
      Un vecteur de norme 1 est dit un vecteur unitaire.
\end{definition}
\begin{lemma}
      Tout vecteur $x$ peut être transformé en vecteur unitaire avec la formule $\hat{x} = \frac{x}{\norm{x}}$
\end{lemma}

\subsubsection{Distance entre deux vecteurs}
Soit $V$ un espace vectoriel muni d'un produit scalaire
\begin{definition}
      La distance entre $x, y \in V$ est $\text{dist}(x, y) = \norm{x - y}$
\end{definition}
\begin{remark}
      Encore une fois, la distance entre deux vecteurs change par rapport au produit scalaire utilisé
\end{remark}

\subsubsection{Inégalité du produit scalaire}
Soit $V$ un espace vectoriel muni d'un produit scalaire, $u, v \in V$
\begin{theorem}
      L'inégalité de Cauchy-Schwarz est $\left|\scpr{u}{v}\right| \leq \norm{u}\norm{v}$
\end{theorem}
\begin{theorem}
      L'inégalité du triangle est $\norm{u + v} \leq \norm{u} + \norm{v}$
\end{theorem}

\subsection{Bases orthonormales}
Soit $V$ un espace vectoriel muni d'un produit scalaire et $S = \{u_1, u_2, \dots, u_m\} \subset V$
\begin{definition}
      $S$ orthonormal $\iff$
      $u_{k_1} \perp u_{k_2} \ \forall \ k_1 \neq k_2$ et $\norm{u_k} = 1 \ \forall \ k = 1, 2, \dots, m$ 
\end{definition}
\begin{definition}
      Le delta de Kronecker est $\delta_{kj} = \begin{cases}
            0 & \text{si} \ k \neq j \\
            1 & \text{si} \ k = j \\
      \end{cases}$
\end{definition}
\begin{remark}
      Le delta de Kronecker pour $i, j \in \{0, 1, \dots, n\}$ est la matrice identité $n \times n$.
      De plus, on peut utiliser le delta de Kronecker pour simplifier la définition d'un ensemble orthonormé.
\end{remark}
\begin{definition}
      $S$ orthonormal $\iff \scpr{u_k}{u_j} = \delta_{kj} \ \forall \ u_k, u_j \in S$ 
\end{definition}
\begin{definition}
      $B$ est une base orthonormale (ou orthonormée) dans $V$ si $B$ est une base dans $V$ et 
      $B$ est un ensemble orthonormal.
\end{definition}
Soit $B$ une base orthonormale de $V$. Notons le produit scalaire dans $V$ comme $\scpr{\cdot}{\cdot}_V$
\begin{theorem}
      Pour $x, y \in V$, on a $\scpr{x}{y}_V = [x]_B^\dagger [y]_B = \scpr{[x]_B}{[y]_B}_{\C^n}$
\end{theorem}
\begin{remark}
      Il est possible de traduire tout produit scalaire dans $V$ au produit scalaire canonique de
      $\C^n$ à l'aide d'une base orthonormale de $V$.
\end{remark}

\subsection{Projection orthogonale}
Soit $R = \{u_1, u_2, \dots, u_m\} \subset V$ un ensemble orthonormal avec $\dim V = n > m$ \\
Considérons le sous-espace $W = \text{span}\{R\} \subset V$ qui a $R$ comme base orthonormale
\begin{definition}
      Soit $v \in V$, alors $S_W(v) = \sum\limits_{k = 1}^{m}\scpr{u_k}{v}{u_k}$
\end{definition}
\begin{theorem}
      Le vecteur $S_W(v) \in W$ et $S_W(v) \perp w$ $\forall \ v \in V, \ w \in W$
\end{theorem}

\subsection{Orthonormalisation de Gram-Schmidt}
Soit $W = \{w_1, w_2, \dots, w_m\}$ un ensemble linéairement indépendant. L'orthonormalisation 
de Gram-Schmidt va produire un ensemble orthonormal $S = \{u_1, u_2, \dots, u_m\}$ à partir
des vecteurs de $W$. 
\begin{align*}
      &\text{Étape} \ 1. \quad \text{Posons} \ u_1 = \frac{w_1}{\norm{w_1}}& &\text{Alors} \ u_1 \ \text{est unitaire} \\
      &\text{Étape} \ 2. \quad \text{Posons} \ v_2 = w_2 - S_{<u_1>}(w_2)& &\text{Alors} \ v_2 \perp u_1 \\
      &\text{Étape} \ 3. \quad \text{Posons} \ u_2 = \frac{v_2}{\norm{v_2}}& &\text{Alors} \ u_2 \ \text{est unitaire et} \ u_2 \perp u_1 \\
      \vdots \\
      &\text{Étape} \ 2m - 2. \quad \text{Posons} \ v_{m} = w_{m} - S_{<u_1, \dots, u_{m - 1}>}(w_{m})& &\text{Alors} \ v_{m} \perp \{u_1, \dots, u_{m - 1}\} \\
      &\text{Étape} \ 2m - 1. \quad \text{Posons} \ u_{m} = \frac{v_{m}}{\norm{v_{m}}} & &\text{Alors} \ u_{m} \ \text{est unitaire et} \ u_{m} \perp \{u_1, \dots, u_{m - 1}\}
\end{align*}
Le procédé va se terminer quand toutes les $2m - 1$ étapes seront faites. \\
Note: La notation $<u_1, \dots, u_j>$ est une façon plus courte d'écrire $\text{span}\{u_1, \dots, u_j\}$