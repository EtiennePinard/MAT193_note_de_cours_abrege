\unnumsec{Matrices}

\subsection{Définition d'une matrice}
\begin{definition}
    Soit $A$ une matrice de type $m$ par $n$. $A$ est aussi noté $A_{m \times n}$ et peut être représentée
    par un tableau qui contient $m$ lignes et $n$ colonnes ou $\left(A\right)_{kj}$ est l'élément
    à la ligne $k$ et colonne $j$. \[
        A = A_{m \times n} = \begin{pmatrix}
            a_{11} & a_{12}      & \dots  & a_{1n} \\
            a_{21} & a_{22}      & \dots  & a_{2n} \\
            \vdots &             & \ddots & \vdots \\
            a_{m1} & a_{m2}      & \dots  & a_{mn}
        \end{pmatrix}, \; \left(A\right)_{kj} \in F \ \forall \ k, j
    \]
\end{definition}
\begin{definition}
    L'ensemble des matrices de type $m \times n$ sur un corps $F$ est noté $M_{m \times n}(F)$ \\
    Si $F$ est un corps quelconque, alors $M_{m \times n}(F) = M_{m \times n}$
\end{definition}
\begin{note}
    Dans ce cours, on considère seulement le cas $F = \R$ ou $F = \C$
\end{note}
\begin{definition}
    Si $m = n$ alors $A_{m \times n} = A_{nn} = A_n$ est une matrice carrée.
\end{definition}
\begin{definition}
    La matrice composée uniquement de 0 pour un certain type $m \times n$ est écrite $\mathbb{O}$
\end{definition}
\begin{definition}
    La matrice identité de type $n$ est écrite $I_n = I$ avec $(I)_{kj} = \begin{cases}
            1 & \text{si} \ i = j    \\
            0 & \text{si} \ i \neq j
        \end{cases}$
\end{definition}
Soient $A \in M_n(F)$ et $a_{kj} \in F$ où $k, j \in \{0, 1, \dots, n\}$
\begin{definition}
    $A$ est triangulaire supérieure si $(A)_{kj} = \begin{cases}
            a_{kj} & \text{si} \ j \geq k \\
            0      & \text{si} \ j < k
        \end{cases}$
\end{definition}
\begin{definition}
    $A$ est triangulaire inférieure si $(A)_{kj} = \begin{cases}
            a_{kj} & \text{si} \ j \leq k \\
            0      & \text{si} \ j > k
        \end{cases}$
\end{definition}
\begin{remark}
    $A$ est triangulaire si $A$ est triangulaire supérieure ou inférieure
\end{remark}
\begin{definition}
    $A$ est diagonale si $(A)_{kj} = \begin{cases}
            a_{kj} & \text{si} \ i = j    \\
            0      & \text{si} \ i \neq j
        \end{cases}$
    \begin{remark}
        $I_n = \text{diag}\{ 1, 1, \dots, 1 \}$ ainsi que $\mathbb{O}_n = \text{diag}\{ 0, 0, \dots, 0 \}$
    \end{remark}
\end{definition}


\subsection{Opérations matricielles}

\subsubsection{Addition de matrices}
L'addition de deux matrices $A$ et $B$ existe seulement si $A$ et $B$ sont du même type.
L'addition est commutative, associative, possède un élément neutre et possède un inverse.
\[ (AB)_{kj} = a_{kj} + b_{kj} \]

\subsubsection{Multiplication par un scalaire}
La multiplication par un scalaire $\alpha$ existe pour une matrice $A$ de n'importe quel type. La multiplication
par un scalaire est commutative, associative, distributive et possède un inverse si $\alpha \neq 0$.
\[ (\alpha A)_{kj} = \alpha a_{kj}\]

\subsubsection{Produit matricielle}
Soient $A \in M_{m \times n}$ et $B \in M_{p \times q}$. 
Le produit matricielle $AB$ existe si $n = p$. Dans ce cas, la matrice résultante est de type $m \times q$ et l'élément à la position $k, j$ est calculé comme tel
\[ (AB)_{kj} = \sum_{s = 1}^{n} a_{ks} b_{sj} \]
Visuellement, on peut représenter la multiplication de $A$ et $B$ comme suit:

\begin{figure}[!htbp]
\begin{center}
\newcommand{\matMulFigUnit}{1 cm}
\tikzset{
    node style sp/.style={draw,circle,minimum size=\matMulFigUnit},
    node style ge/.style={circle,minimum size=\matMulFigUnit},
}
\begin{tikzpicture}[>=latex]

\matrix (A) [matrix of math nodes,
             nodes = {node style ge},
             left delimiter  = (,
             right delimiter = )] at (-6 * \matMulFigUnit,0)
{
  a_{11} & a_{12} & \ldots & a_{1n}  \\
  |[node style sp]| a_{21} & |[node style sp]| a_{22} & \ldots & |[node style sp]| a_{2n} \\
  \vdots & \vdots & \ddots & \vdots  \\
  a_{m1} & a_{m2} & \ldots & a_{mn}  \\
};
\node [draw,above=10pt] at (A.north)  { $A$ : \textcolor{red}{$m$ lignes} $n$ colonnes};

\matrix (B) [matrix of math nodes,
             nodes = {node style ge},
             left delimiter  = (,
             right delimiter = )] at (0,0)
{
  b_{11} & |[node style sp]| b_{12} & \ldots & b_{1q}  \\
  b_{21} & |[node style sp]| b_{22} & \ldots & b_{2q}  \\
  \vdots & \vdots & \ddots & \vdots  \\
  b_{n1} & |[node style sp]| b_{n2} & \ldots & b_{nq}  \\
};
\node [draw,above=10pt] at (B.north)  { $B$ : $n$ lignes \textcolor{red}{$q$ colonnes}};

\matrix (C) [matrix of math nodes,
             nodes = {node style ge},
             left delimiter  = (,
             right delimiter = )] at (6*\matMulFigUnit,0)
{
  c_{11} & c_{12} & \ldots & c_{1q} \\
  c_{21} & |[node style sp,blue]| c_{22} & \ldots & c_{2q} \\
  \vdots & \vdots & \ddots & \vdots \\
  c_{n1} & c_{n2} & \ldots & c_{nq} \\
};

\path (A)--(B) node[midway, black] {$\times$};
\path (B)--(C) node[midway, black] {$=$};

\draw[blue] (A-2-1.north) -- (A-2-4.north);
\draw[blue] (A-2-1.south) -- (A-2-4.south);
\draw[blue] (B-1-2.west)  -- (B-4-2.west);
\draw[blue] (B-1-2.east)  -- (B-4-2.east);

\node [draw,above=10pt] at (C.north) 
    {$ C = AB$ : \textcolor{red}{$m$ lignes}
                      \textcolor{red}{$q$ colonnes}};

\node [draw, below=10pt] at (B.south)
    {$(C)_{22} = (AB)_{22} = \displaystyle{\sum_{s = 1}^{n} a_{2s} b_{s2}} = a_{21}b_{12} + a_{22}b_{22} + \ldots + a_{2n}b_{n2}$
    };

\end{tikzpicture}
\end{center}
\caption{Représentation de la multiplication de deux matrices\protect\footnotemark[1]}
\end{figure}

\fancyfootnotetext{1}{Le figure vient de \href{https://texample.net/matrix-multiplication/}{https://texample.net/matrix-multiplication/}}

\subsubsection{Propriétés du produit matricielle}
Soient $A, B, C$ des matrices dont le produit matricielle entre eux existe. \\
Le produit matricielle est associatif, distributif et possède un élément neutre, soit $I$. \\
\underline{Important}: Le produit matricielle n'est, généralement, \underline{pas commutatif}
\[
    \begin{matrix}
        (AB)C = A(BC) & AB \neq BA & A(B + C) = AB + AC & (A + B)C = AC + BC & AI = IA = A
    \end{matrix}
\]

\subsubsection{Transposition}
\begin{definition}
    La transposée de $A \in M_{m \times n}$ est notée $A^T$ avec $\left(A^T\right)_{kj} = a_{jk}$.
    \begin{remark}
        Pour une matrice carrée, la transposée est une rotation des anti-diagonales par rapport à la grande diagonale.
    \end{remark}
\end{definition}

\subsubsection{Conjugé hermitien}
\begin{definition}
    Le conjugué hermitien de $A \in M_{m \times n}$ est noté $A^\dagger$ avec $A^\dagger = \left(A^T\right)^\star = \left(A^\star\right)^T$.
    Le conjugué hermitien est aussi appellé la transposée conjugué.
\end{definition}
\begin{remark}
    Pour $A \in M_{m \times n}(\R)$, $A^\dagger = A^T$
\end{remark}

\subsubsection{Propriétés de la transposée et du conjugué hermitien}
Soient $A, B \in M_n$, $\scalaire{\alpha}$. Alors on a les propriétés suivantes:

\begingroup
\renewcommand{\arraystretch}{1.5}

\begin{center}
    
    \begin{tabular}{c@{\hskip 1in} c}
        Involution & Multiplication par un scalaire \\
        $ \left( A^T \right)^T = A$ & $\left( \alpha A \right)^T = \alpha A^T$ \\ 
        $\left( A^\dagger \right)^\dagger = A $ & $\left( \alpha A \right)^\dagger = \alpha^\star A^\dagger$  \\[0.5em]
        Distribution sur l'addition   & Anti-distribution sur la multiplication \\ 
        $\left( A + B \right)^T = A^T + B^T$  & $\left( AB \right)^T = B^TA^T$ \\  
        $\left( A + B \right)^\dagger = A^\dagger + B^\dagger$ & $\left( AB \right)^\dagger = B^\dagger A^\dagger$
    \end{tabular}
\end{center}
\endgroup

\subsubsection{Symétrie, orthogonalité, hermitien et unitaire}
\begin{definition}
    Si $A^T = A$, alors $A$ est dite symétrique. Si $A^T = -A$, alors $A$ est dite anti-symétrique.
\end{definition}
\begin{definition}
    Si $A$ est une matrice carrée telle que $AA^T = A^TA = I$, alors $A$ est dite orthogonale.
\end{definition}
\begin{definition}
    Si $A^\dagger = A$, alors $A$ est dite hermitienne. Si $A^\dagger = -A$, alors $A$ est dite anti-hermitienne.
\end{definition}
\begin{definition}
    Si $A$ est une matrice carrée telle que $AA^\dagger = A^\dagger A = I$, alors $A$ est dite unitaire.
\end{definition}
\begin{remark}
    La symétrie et l'orthogonalité s'appliquent aux matrices réels.
    L'hermitien et l'unitaire s'appliquent aux matrices complexes.
\end{remark}

\subsubsection{Commutateur de matrice}
Soient $A, B \in M_n$, alors le commutateur de $A$ et $B$ est la matrice
\[ \left[ A, B \right] = AB - BA \]

\subsubsection{Propriétés du commutateur}
\begin{enumerate}
    \item $[A, B] = -[B, A]$
    \item $[\alpha A, B ] = [A, \alpha B] = \alpha [A, B]$
    \item $[A + B, C] = [A, C] + [B, C]$
    \item $[A, B]^T = -[A^T, B^T]$
    \item L'identité de Jacobi: $[[A, B], C] + [[C, A], B] + [[B, C], A] = \mathbb{O}$
\end{enumerate}

\subsubsection{Trace d'une matrice}
Soit $A \in M_n(F)$. Alors la trace de $A$ est un élément du coprs $F$ donné par
\[ \text{tr}A =  \sum_{j = 0}^{n} a_{jj} \]

\subsubsection{Propriétés de la trace}
\begin{enumerate}
    \item $\text{tr}\{ AB \} = \text{tr}\{ BA \}$
    \item $\text{tr}\{ A + B \} = \text{tr}A + \text{tr}B$
    \item $\text{tr}\{ \alpha A \} = \alpha \text{tr}A $
    \item $\text{tr}\{ A^T \} = \text{tr}A$, $\text{tr}\{ A^\dagger \} = \text{tr}A^\star$
    \item $\text{tr}\{ [A, B] \} = \text{tr}\{ AB - BA \} = \text{tr}\{ AB \} - \text{tr}\{BA\} = \text{tr}\{ AB \} - \text{tr}\{AB\} = 0$
\end{enumerate}

\subsection{Déterminant d'une matrice carrée}
\begin{definition}
    \label{definition_determinant}
    Le déterminant de $A \in M_{n}(F)$ noté $\det A$ ou $|A|$ est un élément dans le corps $F$ qui peut être caractérisé par trois propriétés:
\begin{enumerate}
    \item Le déterminant de la matrice identité est l'identité dans le corps $F$, c-à-d  \[ \det I = 1 \]
    \item Soit $C_j \in M_{n \times 1}$ une colonne de $A$. Le déterminant de $A$ doit satisfaire la propriété 
    \[ C_j = \alpha w + t \implies \det A = \alpha \det \begin{pmatrix}
                  C_1 & \ldots & w & \ldots & C_n
              \end{pmatrix} + \det \begin{pmatrix}
                  C_1 & \ldots & t & \ldots & C_n
              \end{pmatrix} \]
    \item Soient $C_j$ et $C_k$ deux colonnes de $A$ avec $k \neq j$. Le déterminant de $A$ doit satisfaire la propriété 
    \[ C_j = C_k \implies \det A = 0 \]
\end{enumerate}
\end{definition}
\begin{note}
    Les lignes de $A$ doivent aussi satisfaire les propriétés. 
\end{note}
Il existe plusieurs façons de calculer le déterminant. Ce document en couvre trois.

\subsubsection{Calcul du déterminant par récurrence}
Cette méthode de calculer le déterminant utilise les sous-matrices.
\begin{definition}
    \label{sous_matrice}
    Une sous-matrice d'indice $(k, j)$ de $A \in M_n$ est une matrice notée $M_{kj} \in M_{(n-1) \times (n-1)}$. On obtient $M_{kj}$ en biffant la k-ème ligne et la j-ème colonne de $A$. La sous-matrice est aussi appelée le mineur de $A$.
\end{definition}
Ainsi, on peut calculer le déterminant de $A$ en faisant l'expansion par la k-ème ligne.
\begin{theorem}
    Le calcul du déterminant de $A$ par récurrence sur $n$, la taille de la matrice, est 
    \[
        \det A = \begin{cases}
            a_{11},                                   & \textup{si } n = 1 \\
            \sum_{j = 1}^{n} (-1)^{k + j}a_{kj} \det{M_{kj}} & \textup{si } n > 1
        \end{cases}
    \]
\end{theorem}
\begin{remark}
    $k$ est l'indice d'une ligne de $A$, alors $k$ est un chiffre arbitraire avec la contrainte $1 \leq k \leq n$.
\end{remark}
\begin{corollary}
    Le calcul du déterminant de $A$ en faisant l'expansion par la j-ème colonne est
    \[
        \det A = \begin{cases}
            a_{11},                                   & \textup{si } n = 1 \\
            \sum_{k = 1}^{n} (-1)^{k + j}a_{kj} \det{M_{kj}} & \textup{si } n > 1
        \end{cases}
    \]
\end{corollary}
\begin{remark}
    La seule différence entre l'expansion par une ligne ou une colonne est l'indice de la somme. L'indice est $j$ si on développe par la k-ème ligne, sinon l'indice est $k$ si on développe par la j-ème colonne.
\end{remark}

\subsubsection{Calcul du déterminant par les permutations}
Commençons par définir quelques concepts qui seront utilisés dans cette méthode.
\begin{definition}
    Une permutation des nombres $1, 2, \dots, n$ est un arrangement de ces $n$ nombres sans répétition. Notons $\sigma = \begin{pmatrix}
        \sigma_1 & \sigma_2 & \dots & \sigma_n 
    \end{pmatrix}$ une permutation des $n$ nombres o\`u $\sigma_j \in \{1, \dots, n \}$ et \\ $\sigma_j \neq \sigma_k \iff j \neq k$
\end{definition}
\begin{definition}
    Un couple d'élément $\sigma_j, \sigma_k \in \sigma$ est dit dans le désordre si $j < k$ mais $\sigma_j > \sigma_k$
\end{definition}
\begin{definition}
    Le nombre de désordres d'une permutation $\sigma$ est le nombre de couples de $\sigma$ dans le désordre
\end{definition}
\begin{definition}
    La signature de $\sigma$ est $\text{sign}(\sigma) = (-1)^{\text{nombre de désordre de $\sigma$}}$ 
\end{definition}
\begin{definition}
    $S_n$ est l'ensemble qui contient toutes les permutations des nombres $1, 2, \dots, n$
\end{definition}

\begin{theorem}
    Le calcul du déterminant de $A$ par les permutations est 
    \[
        \det A = \sum_{\sigma \in S_n} \textup{sign}(\sigma) \prod_{i = 1}^{n}a_{j \sigma_j}
    \]
\end{theorem}

\subsubsection{Calcul du déterminant par les propriétés caractéristiques}
Il est possible de prouver des nouvelles propriétés à partir des propriétés caractéristiques du déterminant [\ref{definition_determinant}]. Ces nouvelles propriétés ajoutées aux propriétés caractéristiques nous donnent cinq propriétés qui sont utiles pour calculer le déterminant.
\begin{enumerate}
    \item Échanger deux colonnes ou lignes change le signe du déterminant
          \[\det \arraycolsep=0.3\arraycolsep \begin{pmatrix}
                  C_1 & \ldots & C_j & \ldots & C_k & \ldots & C_n
              \end{pmatrix} = -\det \begin{pmatrix}
                  C_1 & \ldots & C_k & \ldots & C_j & \ldots & C_n
              \end{pmatrix}\]
    \item Si deux colonnes ou lignes sont pareilles, alors le déterminant est nul
          \[\det \arraycolsep=0.3\arraycolsep \begin{pmatrix}
                  C_1 & \ldots & C_j & \ldots & C_j & \ldots & C_n
              \end{pmatrix} = 0\]
    \item Multiplier une colonne ou ligne par un scalaire revient à multiplier le déterminant par ce même scalaire
          \[\det \arraycolsep=0.3\arraycolsep \begin{pmatrix}
                  C_1 & \ldots & \alpha C_j & \ldots & C_n
              \end{pmatrix} = \alpha \det \begin{pmatrix}
                  C_1 & \ldots & C_j & \ldots & C_n
              \end{pmatrix}\]
    \item Ajouter une colonne ou ligne multipliée par un scalaire à une autre colonne ou ligne
          ne change pas le déterminant
          \[ \det \arraycolsep=0.3\arraycolsep \begin{pmatrix}
                  C_1 & \ldots & C_j & \ldots & C_k & \ldots & C_n
              \end{pmatrix} = \det \begin{pmatrix}
                  C_1 & \ldots & C_j + \alpha C_k & \ldots & C_k & \ldots & C_n
              \end{pmatrix}\]
    \item Si on peut décomposer une colonne ou ligne en la somme de deux colonnes ou lignes,
          alors le déterminant est la somme des déterminants des deux matrices qui ont chacune
          une décomposition de la colonne ou ligne
          \[ \det \arraycolsep=0.3\arraycolsep \begin{pmatrix}
                  C_1 & \ldots M + N & \ldots & C_n
              \end{pmatrix} = \det \begin{pmatrix}
                  C_1 & \ldots M & \ldots & C_n
              \end{pmatrix} + \det \begin{pmatrix}
                  C_1 & \ldots N & \ldots & C_n
              \end{pmatrix} \]
\end{enumerate}
Ces cinq propriétés nous permettent de transformer $\det A$ en la forme $\alpha \det I$. Par définition, 
$\det I = 1$, alors nous avons calculé le déterminant de $A$

\subsubsection{Propriétés du déterminant}
Soient $A, B \in M_n, \ \alpha$ scalaire, alors \begin{enumerate}
    \item $\det A^T = \det A$
    \item $\det(\alpha A) = \alpha^n \det(A)$
    \item $\det (A^\star) = (\det A)^\star$
    \item $\det A^\dagger = (\det A)^\star$
    \item $\det(AB) = \det(A) \det(B)$
    \item $\det(A^m) = (\det A)^m$
    \item Généralement, $\det(A + B) \neq \det A + \det B$
    \item $\det(\text{diag}\{a_1, \ a_2, \ldots, a_n \}) = a_1 a_2 \dots a_n$
    \item Si $A$ est triangulaire alors $\det A = a_{11} a_{22} \dots a_{nn}$
\end{enumerate}

\subsection{Matrices inverses}
\begin{definition}
    $A \in M_{n \times n}$ est inversible $\iff \exists \ B$ t.q. $AB = BA = I$. \\
    Alors $B$ est noté $A^{-1}$ et est appellée la matrice inverse de $A$
\end{definition}
\subsubsection{Propriétés de l'inverse}
Soit $A \in M_{n \times n}(\C)$ inversible \begin{enumerate}
    \item $AB = \mathbb{O} \implies B = \mathbb{O}$
    \item Si $AC = BA = I \implies B = BI = B(AC) = (BA)C = IC = C $, l'inverse à gauche et à droite sont égales
    \item $(A^{-1})^{-1} = A$, \quad $(A^T)^{-1} = (A^{-1})^{T}$, \quad $(A^\dagger)^{-1} = (A^{-1})^{\dagger}$
    \item $(AB)^{-1} = B^{-1}A^{-1}$
\end{enumerate}

\subsubsection{Calculer la matrice inverse avec la matrice adjointe}
Commençons par définir les termes qui seront utilisés dans le calcul de l'inverse de $A$
\begin{definition}
     Le cofacteur d'indice $(k, j)$ de $A$, noté $c_{kj}$, est donné par
    \[ c_{kj} = (-1)^{k + j}\det M_{kj} \]    
    où $M_{kj}$ étant la sous-matrice d'indice $(k, j)$ de $A$ de la définition \ref{sous_matrice}
\end{definition}
\begin{definition}
     La matrice adjointe de $A$, notée $\text{adj} A$, est donnée par
\[ \left(\text{adj} A\right)_{kj} = \left( c_{jk} \right) \]
\end{definition}
\begin{theorem}
    Le calcul de l'inverse de $A$ avec sa matrice ajointe et son déterminant est
    \[ A^{-1} = \frac{1}{\det A} \ \textup{adj}A \]
\end{theorem}
Cette manière de calculer la matrice inverse nous permet de formuler ce théorème:
\begin{theorem}
    \label{inversible_small}
    $A$ est inversible $\iff \det A \neq 0$
\end{theorem}

\subsection{Systèmes d'équations linéaires}
\begin{definition}
    Une équation linéaire sur un corps $F$ est une équation de la forme 
    \[ a_1x_1 + a_2x_2 + \ldots + a_nx_n = b, \ a_j, b \in F \]
    On appelle $a_j$ les coefficients, $b$ le terme constant et $x_j$ les inconnus.
\end{definition}
\begin{definition}
    Un système de $m$ d'équations linéaires est un ensemble d'équations linéaires, noté:
    \[ \begin{cases}
            a_{11}x_1 + a_{12}x_2 + \ldots + a_{1n}x_n & = b_1 \\
            a_{21}x_1 + a_{22}x_2 + \ldots + a_{2n}x_n & = b_2 \\
            \vdots                                             \\
            a_{m1}x_1 + a_{m2}x_2 + \ldots + a_{mn}x_n & = b_m
        \end{cases} \]
\end{definition}
\begin{definition}
    Un système linéaire est dit compatible s'il admet au moins une solution et incompatible sinon. Une solution est une matrice de taille $n \times 1$
\end{definition}
\subsubsection{Représentation matricielle d'un système d'équation linéaire}
\begin{theorem}
    Chaque système de $m$ équations linéaires contenant $n$ inconnus peut être représenté par une équation matricielle de la forme $AX = B$ où $A \in M_{m \times n}, \ X \in M_{n \times 1}, \ B \in M_{m \times 1}$, 
    \[
        \begin{cases}
            a_{11}x_1 + a_{12}x_2 + \ldots + a_{1n}x_n & = b_1 \\
            a_{21}x_1 + a_{22}x_2 + \ldots + a_{2n}x_n & = b_2 \\
            \vdots                                             \\
            a_{m1}x_1 + a_{m2}x_2 + \ldots + a_{mn}x_n & = b_m
        \end{cases} \iff \begin{pmatrix}
            a_{11} & a_{12}      & \dots  & a_{1n} \\
            a_{21} & a_{22}      & \dots  & a_{2n} \\
            \vdots & \phantom{a} & \ddots & \vdots \\
            a_{m1} & a_{m2}      & \dots  & a_{mn}
        \end{pmatrix} \begin{pmatrix}
            x_1    \\
            x_2    \\
            \vdots \\
            x_n
        \end{pmatrix} = \begin{pmatrix}
            b_1    \\
            b_2    \\
            \vdots \\
            b_m
        \end{pmatrix}
    \]
\end{theorem}
\begin{definition}
    Dans la forme matricielle d'un système d'équation linéaire, $AX = B$, $A$ est dite la matrice de coefficient, $X$ la matrice d'inconnus et $B$ la matrice de terme constant.
\end{definition}
\begin{definition}
    Dans la forme $AX = B$ d'un système d'équation linéaire, on peut omettre la matrice d'inconnue puisqu'elle est toujours la même dans tous les systèmes linéaires. L'équation matricielle sans la matrice d'inconnue, dite la matrice augmentée du système, est notée $(A|B)$
\end{definition}

\subsubsection{Forme échelonnée d'un système}
\begin{definition}
    Une matrice est de la forme échelonnée si \begin{enumerate}
    \item La ligne qui précède une ligne non-nulle est non-nulle
    \item Le premier coefficient non-nul d'une ligne non-nulle, appelé le pivot, est plus à
          gauche que le pivot de la ligne suivante.
    \end{enumerate}
\end{definition}
La forme échelonnée de la matrice augmentée d'un système linéaire nous donnent un système beaucoup plus simple à résoudre. Pour obtenir la forme échelonnée d'une matrice,
il suffit d'appliquer les opérations élémentaires sur ces lignes. Ces opérations sont \begin{enumerate}
    \item Échanger deux lignes, $L_j \leftrightarrow L_k$
    \item Additionner une ligne à une autre qui est multipliée par un scalaire $L_j \mapsto L_j + \alpha L_k$
    \item Multiplier une ligne par un scalaire non-nul, $L_j \mapsto \alpha L_j, \ \alpha \neq 0$
\end{enumerate}

\subsubsection{Rang d'une matrice}
\begin{definition}
    Le rang de $A$, noté $\text{rg}A$, est le nombre de lignes non-nulles de la forme échelonnée de $A$.
\end{definition}
\begin{lemma}
    On peut utiliser le rang de la matrice échelonnée d'un système à $n$ inconnus pour déterminer si ce système est compatible ou incompatible.
    \begin{enumerate}
        \item Si $\textup{rg}(A) < \textup{rg}(A|B)$, alors le système n'a pas de solution, il est incompatible.
        \item Si $\textup{rg}(A) = \textup{rg}(A|B) = n$, alors le système a une unique solution, il est compatible.
        \item Si $\textup{rg}(A) = \textup{rg}(A|B) < n$, alors le système a une infinité de solutions, il est compatible.
    \end{enumerate}
\end{lemma}
\begin{remark}
    Si $A^\prime$ est la forme échelonnée de $A$, alors $\det A = \alpha \det A^\prime, \ \alpha 
 \ \text{scalaire}$.
    On peut donc étendre le théorème \ref{inversible_small} avec le rang.
\end{remark}
\begin{theorem}
    \label{theorem_extended_once}
    $\det A \neq 0 \iff A $ est inversible $\iff \textup{rg}(A) = n \iff AX=B $ a une unique solution
\end{theorem}
\begin{remark}
    L'unique solution dans ce cas est $X = A^{-1}B$, puisque $A$ est inversible.
\end{remark}

\subsubsection{Calculer l'inverse d'une matrice}
Il est possible de calculer l'inverse de $A$ en échelonnant le système $(A|I)$. Le
système échelonné va donner $(A|I) \sim (I|B)$, avec $B = A^{-1}$.

\subsubsection{Système homogène}
\begin{definition}
    Un système homogène est un système de la forme $AX = \mathbb{O}$
\end{definition}
\begin{remark}
    Un tel système est toujours compatible avec la solution triviale $X = \mathbb{O}$
\end{remark}
\begin{theorem}
    Soit le système homogène $AX = \mathbb{O}$. Si la matrice $A$ est inversible, alors le système a seulement la solution triviale, sinon il contient une infinité de solutions.
\end{theorem}

\subsubsection{Noyau d'un système}
\begin{definition}
    Le noyau de $A \in M_{m \times n}$, noté $\text{N}(A)$, est l'ensemble de toutes les solutions du système homogène
    $AX = \mathbb{O}$, soit 
    \[ \text{N}(A) = \left\{ X \in M_{n \times 1} \ | \ AX = \mathbb{O}  \right\} \]
\end{definition}
On peut reformuler le théorème \ref{theorem_extended_once} avec les systèmes homogènes
\begin{theorem}
    \label{inversible_rank_kernel_thm}
    $\det A \neq 0 \iff A $ est inversible $\iff \textup{rg}(A) = n \iff \textup{N}(A) = \mathbb{O} $
\end{theorem}